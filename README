# LLM Extractor tool

This tool helps to run a model over a bunch of text files, trying to extract structured fields from these files and put them into a database file.
All configuration paremeters, including structured data to look for, is in config.yaml
Most script-related parameters can be overridden in command line, see --help
Your OpenRouter API key is secret and should be in .env file, like this:
    OPENROUTER_API_KEY="sk-or-v1-............................"

## Models, sufficient for this job
- deepseek/deepseek-chat:floor
- qwen/qwen-2.5-72b-instruct:floor is faster and thrice cheaper
- google/gemini-2.0-flash-001:floor is as cheap, and even faster, and seems to make better choices
":floor" suffix means that OpenRouter should pick up cheapest providers for the model, you can drop it or even use ":nitro" if you want speed instead.
Full list of potentially usable models is here https://openrouter.ai/models
But lesser models don't always follow instructions to just return structured JSON without external formatting and notes. This will result in DATA not being filled (although you can still read what model replied in REQUEST_LOG)

## Free local models
If your machine can run sufficiently powerful local models via Ollama https://ollama.com/search, you can do it for free.
Just use something like --provider http://localhost:11434/v1 --model deepseek-r1:7b --timeout 60 (or whatever model and timeout would work for you, i.e. would be filling DATA table)


## Customization
Just edit the list of nodes in config.yaml - description line tells the model what to put in that field.
Notes field is not saved in db as it's intended as a mini chain-of-thought space for the model.

## Parallel execution
It takes random files from source directory, until all are processed (i.e. DATA table has records for all file-chunk pairs). 
You can interrupt it with Ctrl-C and continue later from the next random file.
It does seem to run fine with two copies in parallel, and while looking at the db with DB Browser. I could even edit it and Write Changes - and that doesn't interfere with the run.

## Example project
Sample data is taken from public data project of Vancouver, BC.
https://opendata.vancouver.ca/explore/dataset/public-art/information/
It's originally in CSV where some cells contain large blocks of text.
Because this version works with plain text files as sources, there's an utility to convert csv to that.
The idea in this example is to analyze which public art objects are related to first nations, and which are instagrammable. The model doesn't grasp relevancy part though - I need to work on the prompt.
This is just to give you an idea how this tool can be used.

## Working with results
1. Use free "DB Browser for SQLite" app to open .db file and browse it, and export to csv to work with the data in Excel or Google Sheets later
2. Chunk field is designed for cutting files in chunks that don't fit context window of the model. With large enough window of modern models, it fits fully, so it normally always single chunk number 0 for short enough files. You can overwrite this field in DB Browser with a comment instead of number (don't forget to Write Changes!), then the script will consider this file/chunk unprocessed, and run it again. You can compare results for several models this way.

## Credits
Most of the coding was done by Sonnet 3.5, some by Gemini. Hence there can be parts that are very wrong, I'm not a professional developer, I'm an analyst.

## ToDo
- Remove src directory name from file names in the db? - useful for debug though, when testing on shorter sets of same files in diff directories
- Handle when model prefixes response with "here's your json" - nemotron does that
- Make help show defaults
- How is format field from yaml nodes actually used?
- Support directly reading from csv source